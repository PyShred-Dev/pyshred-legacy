{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from misc.example_helper import *\n",
    "import importlib\n",
    "import processing\n",
    "import models\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "importlib.reload(processing)\n",
    "importlib.reload(models)\n",
    "from processing.data_manager import SHREDDataManager\n",
    "from processing.parametric_data_manager import ParametricSHREDDataManager\n",
    "from models import models\n",
    "from models.shred_models import SHRED\n",
    "\n",
    "C:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\docs\\docs\\source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_traj: (2000, 257, 256)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Non-Parametric Case - train/val/test split by timesteps\n",
    "single_traj = np.load('data3fields_npz/ni_3D.npz')\n",
    "single_traj = single_traj[single_traj.files[0]]\n",
    "single_traj = np.transpose(single_traj, (2,0,1))\n",
    "print('single_traj:', single_traj.shape)\n",
    "manager = SHREDDataManager(lags = 20, train_size = 0.7, val_size = 0.15, test_size = 0.15, scaling = \"minmax\", compression = 20, reconstructor=True, forecastor=True)\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    random_sensors = 3,\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    stationary_sensors= [(0,5), (5,0)],\n",
    "    time=np.arange(0, 2000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1399, 21, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.forecaster.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1399, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.forecaster.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([299, 21, 5])\n",
      "torch.Size([299, 5])\n"
     ]
    }
   ],
   "source": [
    "print(val_set.forecaster.X.shape)\n",
    "print(val_set.forecaster.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([299, 21, 5])\n",
      "torch.Size([299, 5])\n"
     ]
    }
   ],
   "source": [
    "print(test_set.forecaster.X.shape)\n",
    "print(test_set.forecaster.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True])\n",
      "tensor([True, True, True, True, True])\n",
      "tensor([True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(test_set.forecaster.X[1,-1,:] == test_set.forecaster.Y[0,:])\n",
    "print(val_set.forecaster.X[1,-1,:] == val_set.forecaster.Y[0,:])\n",
    "print(train_set.forecaster.X[1,-1,:] == train_set.forecaster.Y[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4656, 0.4639, 0.7151, 0.5001, 0.3212])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.forecaster.Y[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Reconstructor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:00<00:00, 62.56batch/s, loss=0.14, L2=0.518, val_loss=0.0133, val_L2=0.183]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:00<00:00, 103.12batch/s, loss=0.00667, L2=0.125, val_loss=0.000707, val_L2=0.0422]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:00<00:00, 95.89batch/s, loss=0.00216, L2=0.0734, val_loss=0.000101, val_L2=0.0159] \n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:00<00:00, 97.82batch/s, loss=0.00169, L2=0.0652, val_loss=3.66e-5, val_L2=0.00959] \n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:00<00:00, 100.14batch/s, loss=0.0016, L2=0.0635, val_loss=1.25e-5, val_L2=0.00561]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:00<00:00, 64.44batch/s, loss=0.00154, L2=0.0622, val_loss=7.44e-6, val_L2=0.00433]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:00<00:00, 89.22batch/s, loss=0.00149, L2=0.0611, val_loss=1.65e-5, val_L2=0.00645]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:00<00:00, 107.34batch/s, loss=0.00143, L2=0.06, val_loss=2.24e-5, val_L2=0.0075]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:00<00:00, 108.96batch/s, loss=0.00143, L2=0.0599, val_loss=1.88e-5, val_L2=0.00687]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:00<00:00, 99.36batch/s, loss=0.00134, L2=0.058, val_loss=3.39e-5, val_L2=0.00923] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size 5\n",
      "output_size 5\n",
      "\n",
      "Fitting Forecaster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:00<00:00, 94.93batch/s, loss=0.0719, L2=0.477, val_loss=0.0305, val_L2=0.314]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:00<00:00, 102.57batch/s, loss=0.0272, L2=0.317, val_loss=0.0232, val_L2=0.274]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:00<00:00, 108.08batch/s, loss=0.0263, L2=0.312, val_loss=0.021, val_L2=0.261]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:00<00:00, 107.73batch/s, loss=0.0263, L2=0.311, val_loss=0.0204, val_L2=0.257]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:00<00:00, 102.37batch/s, loss=0.026, L2=0.31, val_loss=0.0202, val_L2=0.255]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:00<00:00, 104.04batch/s, loss=0.0259, L2=0.309, val_loss=0.0205, val_L2=0.257]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:00<00:00, 101.92batch/s, loss=0.0261, L2=0.31, val_loss=0.0199, val_L2=0.254]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:00<00:00, 105.61batch/s, loss=0.0262, L2=0.311, val_loss=0.0203, val_L2=0.256]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:00<00:00, 94.25batch/s, loss=0.0262, L2=0.311, val_loss=0.0221, val_L2=0.267]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:00<00:00, 106.68batch/s, loss=0.0262, L2=0.311, val_loss=0.0216, val_L2=0.264]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.18292472, 0.04216024, 0.01591722, 0.00958682, 0.00560595,\n",
       "        0.00432521, 0.00644511, 0.00749968, 0.00687222, 0.00922802],\n",
       "       dtype=float32),\n",
       " array([0.31393322, 0.2736822 , 0.26062334, 0.25669444, 0.25526607,\n",
       "        0.25716653, 0.25383165, 0.25632235, 0.2670341 , 0.26433328],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shred = SHRED(sequence='LSTM', decoder='SDN')\n",
    "shred.fit(train_set, val_set, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_traj: (2000, 257, 256)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n",
      "self.full_state_data flattened (2000, 65792)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m manager \u001b[38;5;241m=\u001b[39m SHREDDataManager(lags \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m, scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, compression \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, reconstructor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, forecastor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m manager\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m single_traj,\n\u001b[0;32m      9\u001b[0m     random_sensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     10\u001b[0m     time\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle_traj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstationary_sensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# multi_traj = np.stack((single_traj, single_traj, single_traj,single_traj), axis=0)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\data_manager.py:71\u001b[0m, in \u001b[0;36mSHREDDataManager.add\u001b[1;34m(self, data, random_sensors, stationary_sensors, mobile_sensors, compression, id, scaling, time)\u001b[0m\n\u001b[0;32m     59\u001b[0m data_processor \u001b[38;5;241m=\u001b[39m SHREDDataProcessor(\n\u001b[0;32m     60\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m     61\u001b[0m     random_sensors\u001b[38;5;241m=\u001b[39mrandom_sensors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstructor_flag:\n\u001b[1;32m---> 71\u001b[0m     dataset_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstructor_data_elements\u001b[38;5;241m.\u001b[39mappend(dataset_dict)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecastor_flag:\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\data_processor.py:77\u001b[0m, in \u001b[0;36mSHREDDataProcessor.generate_dataset\u001b[1;34m(self, train_indices, val_indices, test_indices, method)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself.full_state_data flattened\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# fit (fit and transform can be combine with a wrapper or just integrate the code together)\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# transform\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(method)\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\data_processor.py:143\u001b[0m, in \u001b[0;36mSHREDDataProcessor.fit\u001b[1;34m(self, train_indices, method)\u001b[0m\n\u001b[0;32m    141\u001b[0m full_state_data_std_scaled \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_data)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# rSVD\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m U, S, V \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_svd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_state_data_std_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_components\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV.shape\u001b[39m\u001b[38;5;124m'\u001b[39m, V\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\sklearn\\utils\\extmath.py:524\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    522\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 524\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    533\u001b[0m B \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m M\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\sklearn\\utils\\extmath.py:336\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m    335\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m normalizer(A \u001b[38;5;241m@\u001b[39m Q)\n\u001b[1;32m--> 336\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n\u001b[0;32m    340\u001b[0m Q, _ \u001b[38;5;241m=\u001b[39m qr_normalizer(A \u001b[38;5;241m@\u001b[39m Q)\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\scipy\\linalg\\_decomp_lu.py:201\u001b[0m, in \u001b[0;36mlu\u001b[1;34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124millegal value in \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mth argument of internal gesv|posv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    198\u001b[0m                      \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m-\u001b[39minfo)\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlu\u001b[39m(a, permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    202\u001b[0m        p_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    Compute LU decomposition of a matrix with partial pivoting.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite(a) \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Non-Parametric Case - train/val/test split by timesteps\n",
    "single_traj = np.load('data3fields_npz/ni_3D.npz')\n",
    "single_traj = single_traj[single_traj.files[0]]\n",
    "single_traj = np.transpose(single_traj, (2,0,1))\n",
    "print('single_traj:', single_traj.shape)\n",
    "manager = SHREDDataManager(lags = 20, train_size = 0.7, val_size = 0.15, test_size = 0.15, scaling = \"minmax\", compression = 20, reconstructor=True, forecastor=True)\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    random_sensors = 3,\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    stationary_sensors= [(0,5), (5,0)],\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "\n",
    "# multi_traj = np.stack((single_traj, single_traj, single_traj,single_traj), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = np.load('data3fields_npz/ni_3D.npz')\n",
    "# sample = sample[sample.files[0]]\n",
    "# sensor_perimeter_walk_coordinates = perimeter_walk(height = sample.shape[0], width = sample.shape[1], timesteps = sample.shape[2], clockwise=True)\n",
    "# multi_traj = np.stack((sample, sample, sample), axis=0)\n",
    "# print('multi_traj:', multi_traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_traj = np.transpose(multi_traj, (0,3,1,2))\n",
    "# print('multi_traj:', multi_traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the frame to plot (shape: (257, 256))\n",
    "trajectory_idx = 0\n",
    "time_idx = 300\n",
    "frame = multi_traj[trajectory_idx, time_idx, :, :]\n",
    "\n",
    "# Plot the frame\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame, cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.colorbar(label=\"Intensity\")\n",
    "plt.title(f\"Trajectory {trajectory_idx}, Frame {time_idx}\")\n",
    "plt.xlabel(\"Spatial Dimension 1\")\n",
    "plt.ylabel(\"Spatial Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "ntraj = multi_traj.shape[0]\n",
    "ntime = multi_traj.shape[1]\n",
    "nparams = 1\n",
    "\n",
    "# Create the array\n",
    "params = np.arange(ntraj).reshape(ntraj, 1, 1) * np.ones((1, ntime, nparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Case - train/val/test split by parameter\n",
    "\n",
    "manager = ParametricSHREDDataManager(lags = 20, train_size = 0.5, val_size = 0.25, test_size = 0.25, scaling = \"minmax\", compression = 20)\n",
    "manager.add(\n",
    "    data=multi_traj,\n",
    "    random_sensors=2,\n",
    "    time=np.arange(0, 2000),\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.forecaster == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reconstructor.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train X:',train_set.reconstructor.X.shape)\n",
    "print('train Y:',train_set.reconstructor.Y.shape)\n",
    "\n",
    "print('valid X:',val_set.reconstructor.X.shape)\n",
    "print('valid Y:',val_set.reconstructor.Y.shape)\n",
    "\n",
    "print('test X:',test_set.reconstructor.X.shape)\n",
    "print('test Y:',test_set.reconstructor.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = SHREDDataManager(parametric = False, lags = 20, train_size = 0.7, val_size = 0.15, test_size = 0.15, scaling = \"minmax\", compression = 20, reconstructor=True, forecastor=True)\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    random_sensors = 3,\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    stationary_sensors= [(0,5), (5,0)],\n",
    "    time=np.arange(0, 2000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Case\n",
    "manager = SHREDDataManager(lags=20, train_size=0.7, val_size=0.15, test_size=0.15, scaling=\"minmax\", compression=20, )\n",
    "manager.add(\n",
    "    data=multi_traj,\n",
    "    random_sensors=3,\n",
    "    multi_traj=True,\n",
    "    compression=20,\n",
    "    scaling=\"minmax\",\n",
    "    time=np.arange(0, 2000)  # Assuming all files have 2000 timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Case - train/val/test split by parameter\n",
    "\n",
    "manager = ParametricSHREDDataManager(lags = 20, train_size = 0.5, val_size = 0.25, test_size = 0.25, scaling = \"minmax\", compression = 20)\n",
    "manager.add(\n",
    "    data=multi_traj,\n",
    "    random_sensors=2,\n",
    "    time=np.arange(0, 2000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reconstructor.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_SHREDDataManager_init():\n",
    "#     pass\n",
    "\n",
    "# def validate_SHREDDataManger_add_field():\n",
    "#     pass\n",
    "\n",
    "# def validate_SHREDDataProcessor_init():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Initialize SHREDDataManager\n",
    "manager = SHREDDataManager(lags=20, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "\n",
    "# Folder containing .npz files\n",
    "input_folder = \"data3fields_npz\"\n",
    "\n",
    "# Process each .npz file\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".npz\") and filename != 'ni_3D.npz':\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Different fields need same number of timesteps or else some timesteps have 3 sensors while others have 5 sensors\n",
    "        # hence different fields need same number of trajectories # else some trajectories have 3 sensors while others have 5 sensors, not gonna work.\n",
    "        # number of sensors need to be consistent across all time and across all trajectories\n",
    "\n",
    "        # manager.add_field(sensor_locs, n_params, n_trajectories???, ..., {id}) # since different fields need the same number of trajectories\n",
    "        # actually not necessarily... if field 1 has 1000 timesteps (think of multiple trajects just as more timesteps, though not the same especially with lag generation),\n",
    "        # then field 2 also needs 1000 timesteps for this to work right? Yes. Spatial shape can vary, but not timesteps I believe.\n",
    "\n",
    "        # but sensor locs and n_params shouldn't be in add_field because it might not have those (want to reconstruct a field we aren't measuring from a field we are measuring)\n",
    "        # though time and n_trajectories should remain constant across all fields.\n",
    "\n",
    "        # maybe:\n",
    "        # allow user to create SHREDDataProcessor object, add trajectories needed, then put into Manager. WAIT WHAT ABOUT TRAIN/VALID/TEST indices?\n",
    "        # \n",
    "\n",
    "        # manager.{id}.add_trajectory()\n",
    "        # BADDDDD SEE NOTES AND PHOTOS\n",
    "\n",
    "        # Add the file to the SHREDDataManager\n",
    "        manager.add_field(\n",
    "            file_path=[file_path, file_path, file_path], # each element is a trajectory. Manager ensures all trajectories are number of timesteps, and all fields have same number of trajectories\n",
    "            compression=20,\n",
    "            scaling=\"minmax\",\n",
    "            time=np.arange(0, 2000)  # Assuming all files have 2000 timesteps\n",
    "        )\n",
    "        print(f\"Processed: {filename}\")\n",
    "\n",
    "filename = 'ni_3D.npz'\n",
    "file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "manager.add_field(\n",
    "    file_path=[file_path, file_path, file_path],\n",
    "    random_sensors=3,\n",
    "    stationary_sensors=(7, 9),  # Example coordinates\n",
    "    mobile_sensors=sensor_perimeter_walk_coordinates,\n",
    "    compression=20,\n",
    "    scaling=\"minmax\",\n",
    "    time=np.arange(0, 2000)  # Assuming all files have 2000 timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reconstructor.X.shape[0] + val_set.reconstructor.X.shape[0] + test_set.reconstructor.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be (1400, 21, 5)\n",
    "train_set.forecaster.X[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "shred = models.SHRED(train_set.reconstructor.X.shape[-1], train_set.reconstructor.Y.shape[-1], hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
    "validation_errors = models.fit(shred, train_set.reconstructor, val_set.reconstructor, batch_size=64, num_epochs=1000, lr=1e-3, verbose=True, patience=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = shred(test_set.reconstructor.X).detach().cpu().numpy()\n",
    "prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyshred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
