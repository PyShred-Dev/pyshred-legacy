{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyshred\n",
    "from pyshred.models import SHRED\n",
    "from pyshred.sequence_models import *\n",
    "from pyshred.decoder_models import *\n",
    "from pyshred.datasets import load_plasma # import plasma dataset\n",
    "from pyshred_pypi_helper import *\n",
    "import numpy as np\n",
    "from pyshred.data_processor import SHREDPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(100, size=1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Jex'][(5,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma_data = load_plasma()\n",
    "data_Jex = plasma_data['Jex']\n",
    "data_Jey = plasma_data['Jey']\n",
    "data_Jez = plasma_data['Jez']\n",
    "\n",
    "data = {\n",
    "    'Jex' : data_Jex,\n",
    "    'Jey' : data_Jey,\n",
    "    'Jez' : data_Jez,\n",
    "}\n",
    "\n",
    "Jez_mobile_clockwise_perimeter_walk_cw = perimeter_walk(height = data_Jez.shape[0], width = data_Jez.shape[1], timesteps = data_Jez.shape[2], clockwise=True)\n",
    "Jez_mobile_clockwise_perimeter_walk_ccw = perimeter_walk(height = data_Jez.shape[0], width = data_Jez.shape[1], timesteps = data_Jez.shape[2], clockwise=False)\n",
    "\n",
    "sensors = {\n",
    "    'Jez' : [Jez_mobile_clockwise_perimeter_walk_cw, Jez_mobile_clockwise_perimeter_walk_ccw], # mobile sensors\n",
    "    'Jey' : [(0,0), (49,59)], # selected stationary sensors\n",
    "    'Jex' : 3 # random stationary sensors\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea\n",
    "shred_data_loader = SHREDPreprocessor(lags = 20, compression = True, n_components = None, scaling = \"minmax\")\n",
    "X_train_recon, X_test_recon, y_train_recon, y_test_recon = shred_data_loader.train_test_split(sensor_locations_dict=sensors, full_state_dict=data, train_size=0.8, method = \"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "shred_data_loader = SHREDPreprocessor(lags=20, compression=True, n_components=None, scaling=\"minmax\")\n",
    "\n",
    "# Preprocess raw data (sensor locations -> measurements, generate lagged data)\n",
    "sensor_measurements, sensor_summary = shred_data_loader.preprocess(sensor_locations_dict=sensors, full_state_dict=data)\n",
    "\n",
    "# Generate lagged sequences\n",
    "lagged_sequences = shred_data_loader.generate_lagged_sequences(sensor_measurements=sensor_measurements)\n",
    "\n",
    "# Split into train/test datasets # here \n",
    "X_train, X_test, y_train, y_test = shred_data_loader.train_test_split(\n",
    "    lagged_sequences=lagged_sequences, full_state_dict=data, train_size=0.8, method=\"random\"\n",
    ")\n",
    "\n",
    "# Fit the preprocessor\n",
    "shred_data_loader.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Transform train/test data\n",
    "# X_train_transformed_tensor, flattened_y_train_transformed_tensor = shred_data_loader.transform(X_train, y_train) # here it becomes flattened and concatenated (regardless of fit being called)!!! \n",
    "# (if fit is called do scaling and compression otherwise warn user no scaling or compression skipped)\n",
    "train_dataset_torch = shred_data_loader.transform(X_train, y_train)\n",
    "# X_test_transformed_tensor, y_test_transformed_tensor = shred_data_loader.transform(X_test, y_test)\n",
    "test_dataset_torch = shred_data_loader.transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shred_data_loader = SHREDPreprocessor(lags = 20, compression = True, n_components = None, scaling = \"minmax\")\n",
    "\n",
    "sensor_measurements, sensor_summary = shred_data_loader.sensor_loc_to_measurement(sensor_locations_dict=sensors, full_state_dict=data)\n",
    "X_train, X_test, y_train, y_test = shred_data_loader.generate_datasets(sensor_measurements = sensor_measurements, full_state_dict=data, train_size=0.8, method = \"random\")\n",
    "X = shred_data_loader.generate_datasets(sensor_measurements = sensor_measurements)\n",
    "shred_data_loader.fit(X = X)\n",
    "shred_data_loader.fit(y = y)\n",
    "shred_data_loader.fit(X = y, Y = y)\n",
    "X_transformed, y_transformed = shred_data_loader.transform(X, y)\n",
    "\n",
    "lagged_sequences, sensor_summary = shred_data_loader.generate_lagged_sequences(sensor_locations_dict=sensors, full_state_dict=data)\n",
    "X_train_recon, X_test_recon, y_train_recon, y_test_recon = shred_data_loader.train_test_split(lagged_sequences=lagged_sequences, full_state_dict=data, train_size=0.8, method = \"random\")\n",
    "X_train_forecast, X_test_forecast, y_train_forecast, y_test_forecast = shred_data_loader.train_test_split(lagged_sequences=lagged_sequences, full_state_dict=data, train_size=0.8, method = \"sequential\")\n",
    "X_train_recon_transformed, y_train_recon_transformed = shred_data_loader.fit_transform(X_train_recon, y_train_recon)\n",
    "X_test_recon_transformed, y_test_recon_transformed = shred_data_loader.fit_transform(X_test_recon, y_test_recon)\n",
    "shred_recon = SHRED_RECONSTRUCTOR(sequence = 'LSTM', decoder = 'SDN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1990, 11, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lagged_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in sensors:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sensor_locations(full_state, num_sensors):\n",
    "    spatial_shape = full_state.shape[:-1]  # last dimension always number of timesteps\n",
    "    spatial_points = np.prod(spatial_shape)\n",
    "    sensor_indices = np.random.choice(spatial_points, size=num_sensors, replace=False)\n",
    "    sensor_locations = []\n",
    "\n",
    "    for sensor_index in sensor_indices:\n",
    "        sensor_location = []\n",
    "        for dim in reversed(spatial_shape):\n",
    "            sensor_location.append(sensor_index % dim)\n",
    "            sensor_index //= dim\n",
    "        sensor_location = tuple(reversed(sensor_location))\n",
    "        sensor_locations.append(sensor_location)\n",
    "    \n",
    "    return sensor_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_random_sensor_locations(full_state, num_sensors):\n",
    "    spatial_shape = full_state.shape[:-1] # last dimension always number of timesteps\n",
    "    spatial_points = np.prod(spatial_shape)\n",
    "    sensor_indices = np.random.choice(spatial_points, size = num_sensors, replace = False)\n",
    "    sensor_locations = []\n",
    "    for sensor_index in sensor_indices:\n",
    "        sensor_location = []\n",
    "        for dim in reversed(spatial_shape):\n",
    "            sensor_location.append(sensor_index % dim)\n",
    "            sensor_index //= dim\n",
    "        sensor_location = tuple(reversed(sensor_location))\n",
    "        sensor_locations.append(sensor_location)\n",
    "    return sensor_locations\n",
    "\n",
    "def sensor_locations_dict_to_sensor_measurements(full_state_dict, sensor_locations_dict):\n",
    "    sensor_summary = []\n",
    "    sensor_measurements = []\n",
    "    for key, data in sensor_locations_dict.items():\n",
    "        # generate random sensor locations\n",
    "        if isinstance(data, int):\n",
    "            data = generate_random_sensor_locations(full_state = full_state_dict[key], num_sensors = data)\n",
    "        if isinstance(data[0], tuple):\n",
    "            for sensor_coordinate in data:\n",
    "                sensor_summary.append([key, 'stationary', sensor_coordinate])\n",
    "                sensor_measurements.append(full_state_dict[key][sensor_coordinate])\n",
    "        elif isinstance(data[0], list):\n",
    "\n",
    "            for mobile_sensor_coordinates in data:\n",
    "                if len(mobile_sensor_coordinates) != full_state_dict[key].shape[-1]:\n",
    "                    raise ValueError(\n",
    "                        f\"Number of mobile sensor coordinates ({len(mobile_sensor_coordinates)}) \"\n",
    "                        f\"must match the number of timesteps ({full_state_dict[key].shape[-1]}).\"\n",
    "                    )\n",
    "                sensor_summary.append([key, 'mobile', mobile_sensor_coordinates])\n",
    "                sensor_measurements.append([\n",
    "                    full_state_dict[key][sensor_coordinate][timestep]\n",
    "                    for timestep, sensor_coordinate in enumerate(mobile_sensor_coordinates)\n",
    "                ])\n",
    "                # sensor_measurements.append(np.array([full_state_dict[key][sensor_coordinate] for \n",
    "                #                                      sensor_coordinate in mobile_sensor_coordinates]))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported sensor type for key {key}.\")\n",
    "    sensor_measurements = np.array(sensor_measurements)\n",
    "    sensor_summary = pd.DataFrame(sensor_summary, columns=[\"dataset\", \"type\", \"location/trajectory\"])\n",
    "    return {\n",
    "        \"sensor_measurements\": sensor_measurements,\n",
    "        \"sensor_summary\": sensor_summary,\n",
    "    }\n",
    "\n",
    "def generate_lagged_sequences(lags, full_state_dict = None, sensor_locations_dict = None, sensor_measurements = None):\n",
    "    if sensor_measurements is None:\n",
    "        if full_state_dict is None or sensor_locations_dict is None:\n",
    "            raise ValueError(\"Provide either `sensor_measurements` or both `full_state_dict` and `sensor_locations_dict`.\")\n",
    "        sensor_measurements = sensor_locations_dict_to_sensor_measurements(full_state_dict, sensor_locations_dict)[\"sensor_measurements\"]\n",
    "    return generate_lagged_sequences_from_sensor_measurements(sensor_measurements, lags), sensor_locations_dict_to_sensor_measurements(full_state_dict, sensor_locations_dict)[\"sensor_summary\"]\n",
    "\n",
    "\n",
    "def generate_lagged_sequences_from_sensor_measurements(sensor_measurements, lags):\n",
    "    num_sensors = sensor_measurements.shape[0]\n",
    "    num_timesteps = sensor_measurements.shape[1]\n",
    "\n",
    "    if num_timesteps <= lags:\n",
    "        raise ValueError(\"Number of timesteps must be greater than the number of lags.\")\n",
    "\n",
    "    lagged_sequences = np.empty((num_timesteps - lags, lags + 1, num_sensors))\n",
    "    for i in range(lagged_sequences.shape[0]):\n",
    "        lagged_sequences[i] = sensor_measurements[:, i:i+lags+1].T\n",
    "    return lagged_sequences\n",
    "\n",
    "\n",
    "def train_test_split(full_state_dict, lagged_sequences, train_size = 0.8, method = \"random\"):\n",
    "    num_timesteps_minus_lags = lagged_sequences.shape[0]\n",
    "    if train_size <= 0 or train_size >= 1:\n",
    "        raise ValueError(\"`train_size` must be in the range (0.0, 1.0).\")\n",
    "    # generate random indices for train/test\n",
    "    if method == \"random\":\n",
    "        indices = np.random.permutation(num_timesteps_minus_lags)\n",
    "    elif method == \"sequential\":\n",
    "        indices = np.arange(num_timesteps_minus_lags)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method '{method}'. Use 'random' or 'sequential'.\")\n",
    "    num_train_indices = int(train_size * num_timesteps_minus_lags)\n",
    "    train_indices = indices[:num_train_indices]\n",
    "    test_indices = indices[num_train_indices:]\n",
    "    # Inputs\n",
    "    X_train = lagged_sequences[train_indices]\n",
    "    X_test = lagged_sequences[test_indices]\n",
    "    # Outputs\n",
    "    y_train = {}\n",
    "    y_test = {}\n",
    "    for key, full_state_data in full_state_dict.items():\n",
    "        y_train[key] = full_state_data[..., train_indices]\n",
    "        y_test[key] = full_state_data[..., test_indices]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "sensor_measurements, sensor_summary = sensor_locations_dict_to_sensor_measurements(data, sensors)\n",
    "lagged_sequences, sensor_summary = generate_lagged_sequences(lags = 10, full_state_dict = data, sensor_locations_dict = sensors, sensor_measurements = None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, lagged_sequences, train_size = 0.8, method = \"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>type</th>\n",
       "      <th>location/trajectory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jez</td>\n",
       "      <td>mobile</td>\n",
       "      <td>[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jez</td>\n",
       "      <td>mobile</td>\n",
       "      <td>[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jey</td>\n",
       "      <td>stationary</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jey</td>\n",
       "      <td>stationary</td>\n",
       "      <td>(49, 59)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jex</td>\n",
       "      <td>stationary</td>\n",
       "      <td>(10, 48)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jex</td>\n",
       "      <td>stationary</td>\n",
       "      <td>(19, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jex</td>\n",
       "      <td>stationary</td>\n",
       "      <td>(8, 57)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset        type                                location/trajectory\n",
       "0     Jez      mobile  [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5...\n",
       "1     Jez      mobile  [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0...\n",
       "2     Jey  stationary                                             (0, 0)\n",
       "3     Jey  stationary                                           (49, 59)\n",
       "4     Jex  stationary                                           (10, 48)\n",
       "5     Jex  stationary                                            (19, 5)\n",
       "6     Jex  stationary                                            (8, 57)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 60, 398)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test['Jex'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 60, 1592)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['Jex'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lagged_sequences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 60, 2000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Jex'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.45310000e+03, -1.45310000e+03,  8.75030000e+02, ...,\n",
       "          4.87961480e+02,  9.51231320e+01, -3.91650703e+02],\n",
       "        [-3.88715490e+03, -3.52302730e+03, -1.59030000e+02, ...,\n",
       "         -3.14758769e+01, -3.67242173e+01,  1.47297779e+01],\n",
       "        [-4.09875947e+03, -3.52171320e+03, -1.37350000e-01, ...,\n",
       "         -9.08299872e+00, -1.60889589e+00,  3.04896704e+01],\n",
       "        ...,\n",
       "        [-4.05473714e+03, -2.41615624e+03, -1.46460000e+01, ...,\n",
       "         -1.36076704e+01, -1.12196073e+02,  1.17682107e+02],\n",
       "        [-4.45804462e+03, -2.40499854e+03, -1.67140000e+02, ...,\n",
       "          4.75786193e+01, -9.84582751e+01,  1.07320944e+02],\n",
       "        [-4.40876401e+03, -2.25425813e+03, -7.14810000e+00, ...,\n",
       "         -8.38844863e+01, -6.80402050e+01,  9.66751490e+01]],\n",
       "\n",
       "       [[-3.88715490e+03, -3.52302730e+03, -1.59030000e+02, ...,\n",
       "         -3.14758769e+01, -3.67242173e+01,  1.47297779e+01],\n",
       "        [-4.09875947e+03, -3.52171320e+03, -1.37350000e-01, ...,\n",
       "         -9.08299872e+00, -1.60889589e+00,  3.04896704e+01],\n",
       "        [-3.93274418e+03, -3.24417760e+03, -8.28750000e+01, ...,\n",
       "          7.09440477e+00, -6.59029927e+01,  2.79044065e+00],\n",
       "        ...,\n",
       "        [-4.45804462e+03, -2.40499854e+03, -1.67140000e+02, ...,\n",
       "          4.75786193e+01, -9.84582751e+01,  1.07320944e+02],\n",
       "        [-4.40876401e+03, -2.25425813e+03, -7.14810000e+00, ...,\n",
       "         -8.38844863e+01, -6.80402050e+01,  9.66751490e+01],\n",
       "        [-4.46863262e+03, -2.12981365e+03, -1.64610000e+02, ...,\n",
       "         -5.08072049e+01, -1.25302808e+02,  8.87316431e+01]],\n",
       "\n",
       "       [[-4.09875947e+03, -3.52171320e+03, -1.37350000e-01, ...,\n",
       "         -9.08299872e+00, -1.60889589e+00,  3.04896704e+01],\n",
       "        [-3.93274418e+03, -3.24417760e+03, -8.28750000e+01, ...,\n",
       "          7.09440477e+00, -6.59029927e+01,  2.79044065e+00],\n",
       "        [-4.08718462e+03, -3.18741444e+03, -1.64840000e+02, ...,\n",
       "          3.00091547e+01, -5.96236812e+01,  3.40814500e+01],\n",
       "        ...,\n",
       "        [-4.40876401e+03, -2.25425813e+03, -7.14810000e+00, ...,\n",
       "         -8.38844863e+01, -6.80402050e+01,  9.66751490e+01],\n",
       "        [-4.46863262e+03, -2.12981365e+03, -1.64610000e+02, ...,\n",
       "         -5.08072049e+01, -1.25302808e+02,  8.87316431e+01],\n",
       "        [-4.74669686e+03, -2.15713591e+03, -1.23890000e+02, ...,\n",
       "         -8.93876618e+01, -1.59216601e+02,  8.19511320e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.82862668e+03, -2.87180834e+03,  2.22860000e+03, ...,\n",
       "          2.61095972e+03, -2.72644545e+02, -2.68404800e+02],\n",
       "        [-8.76794604e+03, -3.15102702e+03,  1.98030000e+03, ...,\n",
       "          2.59229083e+03, -1.87286832e+02,  3.18948668e+01],\n",
       "        [-8.54018004e+03, -3.23307550e+03,  1.64530000e+03, ...,\n",
       "          2.66732912e+03, -3.39094466e+02,  1.91487984e+01],\n",
       "        ...,\n",
       "        [-6.93003530e+03, -3.86529793e+03, -3.32200000e+02, ...,\n",
       "          6.26104819e+02, -7.87217527e+02,  5.39186408e+02],\n",
       "        [-6.47094970e+03, -4.34392966e+03, -4.92650000e+02, ...,\n",
       "         -1.32353413e+02, -5.86271348e+02,  7.32267757e+02],\n",
       "        [-6.01559794e+03, -4.96078334e+03, -6.89280000e+02, ...,\n",
       "         -7.45457625e+02, -4.47883222e+02,  9.91930041e+02]],\n",
       "\n",
       "       [[-8.76794604e+03, -3.15102702e+03,  1.98030000e+03, ...,\n",
       "          2.59229083e+03, -1.87286832e+02,  3.18948668e+01],\n",
       "        [-8.54018004e+03, -3.23307550e+03,  1.64530000e+03, ...,\n",
       "          2.66732912e+03, -3.39094466e+02,  1.91487984e+01],\n",
       "        [-8.28958559e+03, -3.73959775e+03,  1.25830000e+03, ...,\n",
       "          2.51762802e+03, -6.70568064e+02, -2.55054199e+02],\n",
       "        ...,\n",
       "        [-6.47094970e+03, -4.34392966e+03, -4.92650000e+02, ...,\n",
       "         -1.32353413e+02, -5.86271348e+02,  7.32267757e+02],\n",
       "        [-6.01559794e+03, -4.96078334e+03, -6.89280000e+02, ...,\n",
       "         -7.45457625e+02, -4.47883222e+02,  9.91930041e+02],\n",
       "        [-5.67998076e+03, -5.51671795e+03, -1.14920000e+03, ...,\n",
       "         -1.05488079e+03,  6.40690517e+01,  1.31793936e+03]],\n",
       "\n",
       "       [[-8.54018004e+03, -3.23307550e+03,  1.64530000e+03, ...,\n",
       "          2.66732912e+03, -3.39094466e+02,  1.91487984e+01],\n",
       "        [-8.28958559e+03, -3.73959775e+03,  1.25830000e+03, ...,\n",
       "          2.51762802e+03, -6.70568064e+02, -2.55054199e+02],\n",
       "        [-8.15685020e+03, -3.67727333e+03,  8.97290000e+02, ...,\n",
       "          2.21280588e+03, -8.60177786e+02, -2.50907264e+02],\n",
       "        ...,\n",
       "        [-6.01559794e+03, -4.96078334e+03, -6.89280000e+02, ...,\n",
       "         -7.45457625e+02, -4.47883222e+02,  9.91930041e+02],\n",
       "        [-5.67998076e+03, -5.51671795e+03, -1.14920000e+03, ...,\n",
       "         -1.05488079e+03,  6.40690517e+01,  1.31793936e+03],\n",
       "        [-5.30506425e+03, -6.16054381e+03, -1.50340000e+03, ...,\n",
       "         -1.22512251e+03,  1.46770944e+02,  1.35108271e+03]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lagged_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msensor_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "sensor_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msensor_measurements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "sensor_measurements.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_Jex.shape:', data_Jex.shape)\n",
    "print('type(data_Jex):',type(data_Jex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current ideas:\n",
    "\n",
    "shred_preprocessor = SHREDPreprocessor()\n",
    "X_train, X_test, Y_train, Y_test = shred_preprocessor.fit_transform(data, sensors, train_size = 0.8, lags = 20, split=\"random\", n_components = 20, scaling = True)\n",
    "shred = SHRED(sequence=\"LSTM\", decoder=\"SDN\")\n",
    "shred.fit(X = X_train, y = Y_train, num_epochs=10) # by default: n_components = 20\n",
    "shred.eval(X = X_test, y = Y_test)\n",
    "shred.recon(X)\n",
    "shred_preprocessor.inverse_transform(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shred_preprocessor = SHREDPreprocessor()\n",
    "X_train, X_test, Y_train, Y_test, time = shred_preprocessor.fit_transform(data, sensors, time = None, train_size = 0.8, lags = 20, split=\"random\", n_components = 20, scaling = True)\n",
    "shred = SHRED(sequence=\"LSTM\", decoder=\"SDN\")\n",
    "shred.fit(X = X_train, y = Y_train, time = time, num_epochs=10) # by default: n_components = 20\n",
    "shred.eval(X = X_test, y = Y_test, time = None)\n",
    "shred.predict(X)\n",
    "shred.forecast(t = timesteps_forward)\n",
    "shred_preprocessor.inverse_transform(test_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_and_split(data = data, sensors = sensors, time = None, train_size = 0.8, lags = 20, n_components = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shred_data = DataLoader(data = data, sensors = sensors, time = None)\n",
    "test_data = DataLoader(data = test_set, sensors = sensors, time = None)\n",
    "shred = SHRED(sequence=\"LSTM\", decoder=\"SDN\")\n",
    "shred.fit(X = X_train, y = y_train, num_epochs=10) # by default: n_components = 20\n",
    "shred.eval(X = X_test, y = y_test, time = None)\n",
    "\n",
    "\n",
    "shred.predict(start = start_time, end = end_time)\n",
    "shred.forecast(t = timesteps_forward)\n",
    "\n",
    "SHRED.split_data(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shred_dataset = DataProcessor(data = data, sensors = sensors, time = None)\n",
    "recon_train_dataset, recon_val_dataset, recon_test_dataset, forecast_train_dataset, forecast_val_dataset, forecast_test_dataset = shred_dataset.prepare_datasets(val_size = 0.2, test_size = 0.2, lags = 20, n_components = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# want: train, val, test = shred_dataset.prepare_datasets(val_size = 0.2, test_size = 0.2, lags = 20, n_components = 20)\n",
    "\n",
    "# Next up:\n",
    "\n",
    "# this:\n",
    "# shred.fit(data, sensors, lags = 40, time = None, sensor_forecaster = True, n_components = 20, val_size = 0.2, batch_size=64, num_epochs=4000, lr=1e-3, verbose=True, patience=20):\n",
    "# to:\n",
    "# shred.fit(recon_train, recon_val, forecast_train, forecast_val, batch_size=64, num_epochs=4000, lr=1e-3, verbose=True, patience=20):\n",
    "\n",
    "# create a custom class that inside has multiple standard TimeSeriesDataset objects (wraps multiple together):\n",
    "# shred.fit(train, val, batch_size=64, num_epochs=4000, lr=1e-3, verbose=True, patience=20):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_val_dataset.data_in\n",
    "print('recon_val_dataset.data_in:', recon_val_dataset.data_in.shape) # number time steps, lags + 1, number of sensors\n",
    "print('recon_val_dataset.data_out:', recon_val_dataset.data_out.shape) # number of time steps, 20 components * 3 datasets + 7 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pyshred.prep_and_split(data = data, sensors = sensors, time = None, train_size = 0.8, lags = 20, n_components = 20)\n",
    "shred = SHRED(sequence=sequence_model, decoder=decoder_model)\n",
    "shred.fit(X = X_train, y = y_train, num_epochs=10) # by default: n_components = 20\n",
    "shred.eval(X = X_test, y = y_test, time = None)\n",
    "\n",
    "\n",
    "shred.predict(start = start_time, end = end_time)\n",
    "shred.forecast(t = timesteps_forward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit(self, data, sensors, lags = 40, time = None, sensor_forecaster = True, n_components = 20, val_size = 0.2, batch_size=64, num_epochs=4000, lr=1e-3, verbose=True, patience=20):\n",
    "\n",
    "# def fit(self, recon_train, recon_val, forecast_train, forecast_val, batch_size=64, num_epochs=4000, lr=1e-3, verbose=True, patience=20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_model = Transformer(d_model=64, num_encoder_layers=2, nhead=8, dropout=0.1)\n",
    "decoder_model = SDN(l1 = 200, l2 = 300, dropout= 0.2)\n",
    "shred = SHRED(sequence=sequence_model, decoder=decoder_model)\n",
    "val_errors,  sensor_forecast_errors = shred.fit(data = data, sensors = sensors, num_epochs=10) # by default: n_components = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shred = SHRED(sequence='LSTM', decoder='SDN')\n",
    "shred.fit(data = 'path_to_folder or file_path', sensors = 'default 3', lags = 'default 40', n_components='defuault 20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_model = LSTM(hidden_size=32, num_layers=1)\n",
    "decoder_model = SDN(l1 = 200, l2 = 300, dropout= 0.2)\n",
    "shred = SHRED(sequence=sequence_model, decoder=decoder_model)\n",
    "val_errors,  sensor_forecast_errors = shred.fit(data = data, sensors = sensors, num_epochs=10) # by default: n_components = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shred.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
