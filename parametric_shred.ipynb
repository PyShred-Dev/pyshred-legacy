{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from misc.example_helper import *\n",
    "import importlib\n",
    "import processing\n",
    "import models\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "importlib.reload(processing)\n",
    "importlib.reload(models)\n",
    "from processing.parametric_data_manager import ParametricSHREDDataManager\n",
    "from models.shred_models import SHRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('kuramoto_sivashinsky\\KuramotoSivashinsky_data.npz')\n",
    "data = dataset['u'] # shape (500, 201, 100)\n",
    "mu = dataset['mu'] # shape (500, 201, 2)\n",
    "\n",
    "# # Add data to manager (with sensors)\n",
    "# ni_3D = np.load('data4fields_npz/ni_3D.npz')\n",
    "# ni_3D = ni_3D[ni_3D.files[0]]\n",
    "# mobile_sensors = [\n",
    "#     perimeter_walk(height = ni_3D.shape[1], width = ni_3D.shape[2], timesteps = ni_3D.shape[0], clockwise=True),\n",
    "#     perimeter_walk(height = ni_3D.shape[1], width = ni_3D.shape[2], timesteps = ni_3D.shape[0], clockwise=False)\n",
    "# ]\n",
    "# manager.add(\n",
    "#     data=ni_3D,\n",
    "#     random_sensors=2,\n",
    "#     stationary_sensors=[(7, 9), (0,0)],\n",
    "#     mobile_sensors=mobile_sensors\n",
    "# )\n",
    "\n",
    "# print(f\"Processed: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of indices that alternates back and forth from 0 to 99 until the list length is 201\n",
    "indices = []\n",
    "\n",
    "# Start generating indices\n",
    "while len(indices) < 201:\n",
    "    indices += list(range(100))  # Add 0 to 99\n",
    "    indices += list(range(99, -1, -1))  # Add 99 to 0\n",
    "\n",
    "# Truncate the list to ensure the length is exactly 201\n",
    "indices = indices[:201]\n",
    "\n",
    "print(len(indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_sensors = forward_backward_walk(start=0, end = data.shape[2], timesteps=data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mobile_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train indices [ 52 475 367 260 251 459 350 301 454 307 148 299 391 187  47 112 189 234\n",
      " 143  12 144  37 438 419 316 274  23 366 122 212 128 462 201 480 387 378\n",
      " 288 377 215 317 484 333 348 105 415 386 488 275 290 327 192   0  80 395\n",
      " 404 487 168 423 451 304 242 456 436 121 111 282 211  34 185  56  91 368\n",
      " 173 345 478  90 388 405 401 106 457 413 418 262 305 222 425 232 263  38\n",
      " 342 314 338 256 257  73 442  32 402 252 473  55   5  71 494 102 285  44\n",
      "  57 337  25 155 433 435   8 294  33 409 363   9 110  31 334 369 125 117\n",
      " 492 118 399 188 410 347  94 134  77 383  51 233 280 246 265 312  88 235\n",
      " 474  20 127 254 331 170 427 351 225 283 360  36 167 141 230 499 460 156\n",
      " 470  62 394 182 295 281  83 172 202 209 406 292 446 177  98 268 289 324\n",
      " 214 147  30 149 266 444 176 253 483  66   2 108 380 146 272 113 287  75\n",
      " 207  50 421 213  67 297 164 261 247 319 204 267 114 485 195 277 346 340\n",
      " 320  70  29 339 160   7 379 205 371 440 359  79 308 385 140 107 417  81\n",
      " 332  76 243  17  13 420 464 196 441 310  35 245 479 365   3 430  28 264\n",
      " 490 461 255 428 166  99 300 311   1  78 329  74 159 238 431 357 481  10\n",
      "  54 472 132 200 161 270 469 497 153 416 445 344 400 184 306 120  41 129\n",
      " 221  18  45 217 223 407 296 135  92 361 477 455 486 375 398  85 151  15\n",
      " 450 414 326 467 259 284  24 355 179 240 231 115 298 343 241 466 321  72\n",
      " 315 476  43 482 104 293 123 226 197 330 349 389  84 136 180 131 452 411\n",
      "  27 318 227  48 422 239 463 397]\n",
      "val_indices [322 408 443 471 382 116 465  21 403 248 210  87 291 186  49  42 150 374\n",
      " 152 335  46  95 208 358 220 158 100 191 434 468 412  26 216 336 491 269\n",
      " 302 341 237 139 218 138  82  65  61 165 309 449 373 323  40 190 276  68\n",
      " 437 224  69  14 364 119  64   4 325  86 354 439 124 424 199 286 458 133\n",
      " 244 352 496]\n",
      "test_indices [194 356 372  96  53 154 313 130 258 493 174 328 163 228 162 392 353 362\n",
      " 489 142  89 271 101 370 249  58 381 384 448 396  93 429 219 181 279 183\n",
      " 145 229 278 171  59 495 126   6 193 175 236  39 447 157  11  22 109 376\n",
      " 137 169 178 453 273 393 250  97 426 103 303  63 206 390 432 203  19  60\n",
      " 198  16 498]\n",
      "all_traj_sensor_measurements (500, 201, 4)\n",
      "sensor summary                       sensor type location/trajectory\n",
      "0  stationary (randomly selected)               (20,)\n",
      "1  stationary (randomly selected)               (64,)\n",
      "2      stationary (user selected)                (0,)\n",
      "3      stationary (user selected)                (1,)\n",
      "flattened_valid_traj_sensor_measurements_and_params: (15075, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 422100 into shape (35,2000,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 24\u001b[0m\n\u001b[0;32m     17\u001b[0m mu \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# shape (500, 201, 2)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# mobile_sensors = forward_backward_walk(start=0, end = data[2], timesteps=data[1])\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# mobile_sensors = [\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#     perimeter_walk(height = ni_3D.shape[1], width = ni_3D.shape[2], timesteps = ni_3D.shape[0], clockwise=True),\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#     perimeter_walk(height = ni_3D.shape[1], width = ni_3D.shape[2], timesteps = ni_3D.shape[0], clockwise=False)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_sensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstationary_sensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# mobile_sensors=mobile_sensors,\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\parametric_data_manager.py:77\u001b[0m, in \u001b[0;36mParametricSHREDDataManager.add\u001b[1;34m(self, data, random_sensors, stationary_sensors, mobile_sensors, params, compression, scaling, time, id)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# create and initialize SHREDData object\u001b[39;00m\n\u001b[0;32m     65\u001b[0m data_processor \u001b[38;5;241m=\u001b[39m ParametricSHREDDataProcessor(\n\u001b[0;32m     66\u001b[0m     params \u001b[38;5;241m=\u001b[39m params, \u001b[38;5;66;03m# add method dependent, might add data from a field without params?\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstructor_data_elements\u001b[38;5;241m.\u001b[39mappend(dataset_dict)\n\u001b[0;32m     84\u001b[0m data_processor\u001b[38;5;241m.\u001b[39mdiscard_data()\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\parametric_data_processor.py:177\u001b[0m, in \u001b[0;36mParametricSHREDDataProcessor.generate_dataset\u001b[1;34m(self, train_indices, val_indices, test_indices, method)\u001b[0m\n\u001b[0;32m    172\u001b[0m transformed_flattened_train_traj_sensor_measurements_and_params, transformed_flattened_valid_traj_sensor_measurements_and_params,transformed_flattened_test_traj_sensor_measurements_and_params \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_sensor(flattened_train_traj_sensor_measurements_and_params, flattened_valid_traj_sensor_measurements_and_params,\n\u001b[0;32m    174\u001b[0m                                             flattened_test_traj_sensor_measurements_and_params)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# reshape back to (n_traj, n_time, n_sensors + n_params) to simply lag generating process\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m transformed_train_traj_sensor_measurements_and_params \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_flattened_train_traj_sensor_measurements_and_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformed_flattened_train_traj_sensor_measurements_and_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mntimes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mntimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m transformed_valid_traj_sensor_measurements_and_params \u001b[38;5;241m=\u001b[39m transformed_flattened_valid_traj_sensor_measurements_and_params\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mint\u001b[39m(transformed_flattened_valid_traj_sensor_measurements_and_params\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mntimes), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mntimes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsensors \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnparams)\n\u001b[0;32m    181\u001b[0m transformed_test_traj_sensor_measurements_and_params \u001b[38;5;241m=\u001b[39m transformed_flattened_test_traj_sensor_measurements_and_params\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mint\u001b[39m(transformed_flattened_test_traj_sensor_measurements_and_params\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mntimes), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mntimes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsensors \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnparams)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 422100 into shape (35,2000,6)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize ParametricSHREDDataManager\n",
    "manager = ParametricSHREDDataManager(\n",
    "    lags = 20,\n",
    "    train_size = 0.7,\n",
    "    val_size = 0.15,\n",
    "    test_size = 0.15,\n",
    "    scaling = \"minmax\",\n",
    "    compression = 20,\n",
    "    time=np.arange(0, 2000),\n",
    "    )\n",
    "\n",
    "# Add data to manager (with sensors)\n",
    "dataset = np.load('kuramoto_sivashinsky\\KuramotoSivashinsky_data.npz')\n",
    "data = dataset['u'] # shape (500, 201, 100)\n",
    "mu = dataset['mu'] # shape (500, 201, 2)\n",
    "\n",
    "# mobile_sensors = forward_backward_walk(start=0, end = data[2], timesteps=data[1])\n",
    "# mobile_sensors = [\n",
    "#     perimeter_walk(height = ni_3D.shape[1], width = ni_3D.shape[2], timesteps = ni_3D.shape[0], clockwise=True),\n",
    "#     perimeter_walk(height = ni_3D.shape[1], width = ni_3D.shape[2], timesteps = ni_3D.shape[0], clockwise=False)\n",
    "# ]\n",
    "manager.add(\n",
    "    data=data,\n",
    "    random_sensors=2,\n",
    "    stationary_sensors=[(0,), (1,)],\n",
    "    # mobile_sensors=mobile_sensors,\n",
    "    params=mu,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_traj: (2000, 257, 256)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Non-Parametric Case - train/val/test split by timesteps\n",
    "single_traj = np.load('data3fields_npz/ni_3D.npz')\n",
    "single_traj = single_traj[single_traj.files[0]]\n",
    "single_traj = np.transpose(single_traj, (2,0,1))\n",
    "print('single_traj:', single_traj.shape)\n",
    "manager = SHREDDataManager(lags = 20, train_size = 0.7, val_size = 0.15, test_size = 0.15, scaling = \"minmax\", compression = 20, reconstructor=True, forecastor=True)\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    random_sensors = 3,\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    stationary_sensors= [(0,5), (5,0)],\n",
    "    time=np.arange(0, 2000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1399, 21, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.forecaster.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1399, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.forecaster.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([299, 21, 5])\n",
      "torch.Size([299, 5])\n"
     ]
    }
   ],
   "source": [
    "print(val_set.forecaster.X.shape)\n",
    "print(val_set.forecaster.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([299, 21, 5])\n",
      "torch.Size([299, 5])\n"
     ]
    }
   ],
   "source": [
    "print(test_set.forecaster.X.shape)\n",
    "print(test_set.forecaster.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True])\n",
      "tensor([True, True, True, True, True])\n",
      "tensor([True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(test_set.forecaster.X[1,-1,:] == test_set.forecaster.Y[0,:])\n",
    "print(val_set.forecaster.X[1,-1,:] == val_set.forecaster.Y[0,:])\n",
    "print(train_set.forecaster.X[1,-1,:] == train_set.forecaster.Y[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4656, 0.4639, 0.7151, 0.5001, 0.3212])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.forecaster.Y[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Reconstructor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:00<00:00, 62.56batch/s, loss=0.14, L2=0.518, val_loss=0.0133, val_L2=0.183]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:00<00:00, 103.12batch/s, loss=0.00667, L2=0.125, val_loss=0.000707, val_L2=0.0422]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:00<00:00, 95.89batch/s, loss=0.00216, L2=0.0734, val_loss=0.000101, val_L2=0.0159] \n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:00<00:00, 97.82batch/s, loss=0.00169, L2=0.0652, val_loss=3.66e-5, val_L2=0.00959] \n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:00<00:00, 100.14batch/s, loss=0.0016, L2=0.0635, val_loss=1.25e-5, val_L2=0.00561]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:00<00:00, 64.44batch/s, loss=0.00154, L2=0.0622, val_loss=7.44e-6, val_L2=0.00433]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:00<00:00, 89.22batch/s, loss=0.00149, L2=0.0611, val_loss=1.65e-5, val_L2=0.00645]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:00<00:00, 107.34batch/s, loss=0.00143, L2=0.06, val_loss=2.24e-5, val_L2=0.0075]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:00<00:00, 108.96batch/s, loss=0.00143, L2=0.0599, val_loss=1.88e-5, val_L2=0.00687]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:00<00:00, 99.36batch/s, loss=0.00134, L2=0.058, val_loss=3.39e-5, val_L2=0.00923] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size 5\n",
      "output_size 5\n",
      "\n",
      "Fitting Forecaster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:00<00:00, 94.93batch/s, loss=0.0719, L2=0.477, val_loss=0.0305, val_L2=0.314]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:00<00:00, 102.57batch/s, loss=0.0272, L2=0.317, val_loss=0.0232, val_L2=0.274]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:00<00:00, 108.08batch/s, loss=0.0263, L2=0.312, val_loss=0.021, val_L2=0.261]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:00<00:00, 107.73batch/s, loss=0.0263, L2=0.311, val_loss=0.0204, val_L2=0.257]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:00<00:00, 102.37batch/s, loss=0.026, L2=0.31, val_loss=0.0202, val_L2=0.255]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:00<00:00, 104.04batch/s, loss=0.0259, L2=0.309, val_loss=0.0205, val_L2=0.257]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:00<00:00, 101.92batch/s, loss=0.0261, L2=0.31, val_loss=0.0199, val_L2=0.254]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:00<00:00, 105.61batch/s, loss=0.0262, L2=0.311, val_loss=0.0203, val_L2=0.256]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:00<00:00, 94.25batch/s, loss=0.0262, L2=0.311, val_loss=0.0221, val_L2=0.267]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:00<00:00, 106.68batch/s, loss=0.0262, L2=0.311, val_loss=0.0216, val_L2=0.264]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.18292472, 0.04216024, 0.01591722, 0.00958682, 0.00560595,\n",
       "        0.00432521, 0.00644511, 0.00749968, 0.00687222, 0.00922802],\n",
       "       dtype=float32),\n",
       " array([0.31393322, 0.2736822 , 0.26062334, 0.25669444, 0.25526607,\n",
       "        0.25716653, 0.25383165, 0.25632235, 0.2670341 , 0.26433328],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shred = SHRED(sequence='LSTM', decoder='SDN')\n",
    "shred.fit(train_set, val_set, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_traj: (2000, 257, 256)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n",
      "self.full_state_data flattened (2000, 65792)\n",
      "n_components 20\n",
      "V.shape (20, 65792)\n",
      "compressed full_state_data: (2000, 20)\n",
      "self.full_state_data pre transform (2000, 65792)\n",
      "transformed_data_std_scale (2000, 65792)\n",
      "transformed_data (2000, 20)\n",
      "self.full_state_data flattened (2000, 65792)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m manager \u001b[38;5;241m=\u001b[39m SHREDDataManager(lags \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m, scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, compression \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, reconstructor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, forecastor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m manager\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m single_traj,\n\u001b[0;32m      9\u001b[0m     random_sensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     10\u001b[0m     time\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle_traj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstationary_sensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# multi_traj = np.stack((single_traj, single_traj, single_traj,single_traj), axis=0)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\data_manager.py:71\u001b[0m, in \u001b[0;36mSHREDDataManager.add\u001b[1;34m(self, data, random_sensors, stationary_sensors, mobile_sensors, compression, id, scaling, time)\u001b[0m\n\u001b[0;32m     59\u001b[0m data_processor \u001b[38;5;241m=\u001b[39m SHREDDataProcessor(\n\u001b[0;32m     60\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m     61\u001b[0m     random_sensors\u001b[38;5;241m=\u001b[39mrandom_sensors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstructor_flag:\n\u001b[1;32m---> 71\u001b[0m     dataset_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstructor_data_elements\u001b[38;5;241m.\u001b[39mappend(dataset_dict)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecastor_flag:\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\data_processor.py:77\u001b[0m, in \u001b[0;36mSHREDDataProcessor.generate_dataset\u001b[1;34m(self, train_indices, val_indices, test_indices, method)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself.full_state_data flattened\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# fit (fit and transform can be combine with a wrapper or just integrate the code together)\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# transform\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(method)\n",
      "File \u001b[1;32mc:\\Users\\David\\OneDrive - UW\\2024_2025\\GoodPySHRED\\PyShred\\processing\\data_processor.py:143\u001b[0m, in \u001b[0;36mSHREDDataProcessor.fit\u001b[1;34m(self, train_indices, method)\u001b[0m\n\u001b[0;32m    141\u001b[0m full_state_data_std_scaled \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_data)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# rSVD\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m U, S, V \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_svd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_state_data_std_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_components\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV.shape\u001b[39m\u001b[38;5;124m'\u001b[39m, V\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\sklearn\\utils\\extmath.py:524\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    522\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 524\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    533\u001b[0m B \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m M\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\sklearn\\utils\\extmath.py:336\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m    335\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m normalizer(A \u001b[38;5;241m@\u001b[39m Q)\n\u001b[1;32m--> 336\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n\u001b[0;32m    340\u001b[0m Q, _ \u001b[38;5;241m=\u001b[39m qr_normalizer(A \u001b[38;5;241m@\u001b[39m Q)\n",
      "File \u001b[1;32mc:\\Tools\\MiniConda\\envs\\pyshred\\lib\\site-packages\\scipy\\linalg\\_decomp_lu.py:201\u001b[0m, in \u001b[0;36mlu\u001b[1;34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124millegal value in \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mth argument of internal gesv|posv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    198\u001b[0m                      \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m-\u001b[39minfo)\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlu\u001b[39m(a, permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    202\u001b[0m        p_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    Compute LU decomposition of a matrix with partial pivoting.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite(a) \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Non-Parametric Case - train/val/test split by timesteps\n",
    "single_traj = np.load('data3fields_npz/ni_3D.npz')\n",
    "single_traj = single_traj[single_traj.files[0]]\n",
    "single_traj = np.transpose(single_traj, (2,0,1))\n",
    "print('single_traj:', single_traj.shape)\n",
    "manager = SHREDDataManager(lags = 20, train_size = 0.7, val_size = 0.15, test_size = 0.15, scaling = \"minmax\", compression = 20, reconstructor=True, forecastor=True)\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    random_sensors = 3,\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    stationary_sensors= [(0,5), (5,0)],\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "\n",
    "# multi_traj = np.stack((single_traj, single_traj, single_traj,single_traj), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = np.load('data3fields_npz/ni_3D.npz')\n",
    "# sample = sample[sample.files[0]]\n",
    "# sensor_perimeter_walk_coordinates = perimeter_walk(height = sample.shape[0], width = sample.shape[1], timesteps = sample.shape[2], clockwise=True)\n",
    "# multi_traj = np.stack((sample, sample, sample), axis=0)\n",
    "# print('multi_traj:', multi_traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_traj = np.transpose(multi_traj, (0,3,1,2))\n",
    "# print('multi_traj:', multi_traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the frame to plot (shape: (257, 256))\n",
    "trajectory_idx = 0\n",
    "time_idx = 300\n",
    "frame = multi_traj[trajectory_idx, time_idx, :, :]\n",
    "\n",
    "# Plot the frame\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame, cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.colorbar(label=\"Intensity\")\n",
    "plt.title(f\"Trajectory {trajectory_idx}, Frame {time_idx}\")\n",
    "plt.xlabel(\"Spatial Dimension 1\")\n",
    "plt.ylabel(\"Spatial Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "ntraj = multi_traj.shape[0]\n",
    "ntime = multi_traj.shape[1]\n",
    "nparams = 1\n",
    "\n",
    "# Create the array\n",
    "params = np.arange(ntraj).reshape(ntraj, 1, 1) * np.ones((1, ntime, nparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Case - train/val/test split by parameter\n",
    "\n",
    "manager = ParametricSHREDDataManager(lags = 20, train_size = 0.5, val_size = 0.25, test_size = 0.25, scaling = \"minmax\", compression = 20)\n",
    "manager.add(\n",
    "    data=multi_traj,\n",
    "    random_sensors=2,\n",
    "    time=np.arange(0, 2000),\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.forecaster == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reconstructor.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train X:',train_set.reconstructor.X.shape)\n",
    "print('train Y:',train_set.reconstructor.Y.shape)\n",
    "\n",
    "print('valid X:',val_set.reconstructor.X.shape)\n",
    "print('valid Y:',val_set.reconstructor.Y.shape)\n",
    "\n",
    "print('test X:',test_set.reconstructor.X.shape)\n",
    "print('test Y:',test_set.reconstructor.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = SHREDDataManager(parametric = False, lags = 20, train_size = 0.7, val_size = 0.15, test_size = 0.15, scaling = \"minmax\", compression = 20, reconstructor=True, forecastor=True)\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    random_sensors = 3,\n",
    "    time=np.arange(0, 2000)\n",
    ")\n",
    "manager.add(\n",
    "    data = single_traj,\n",
    "    stationary_sensors= [(0,5), (5,0)],\n",
    "    time=np.arange(0, 2000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Case\n",
    "manager = SHREDDataManager(lags=20, train_size=0.7, val_size=0.15, test_size=0.15, scaling=\"minmax\", compression=20, )\n",
    "manager.add(\n",
    "    data=multi_traj,\n",
    "    random_sensors=3,\n",
    "    multi_traj=True,\n",
    "    compression=20,\n",
    "    scaling=\"minmax\",\n",
    "    time=np.arange(0, 2000)  # Assuming all files have 2000 timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Case - train/val/test split by parameter\n",
    "\n",
    "manager = ParametricSHREDDataManager(lags = 20, train_size = 0.5, val_size = 0.25, test_size = 0.25, scaling = \"minmax\", compression = 20)\n",
    "manager.add(\n",
    "    data=multi_traj,\n",
    "    random_sensors=2,\n",
    "    time=np.arange(0, 2000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reconstructor.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_SHREDDataManager_init():\n",
    "#     pass\n",
    "\n",
    "# def validate_SHREDDataManger_add_field():\n",
    "#     pass\n",
    "\n",
    "# def validate_SHREDDataProcessor_init():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Initialize SHREDDataManager\n",
    "manager = SHREDDataManager(lags=20, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "\n",
    "# Folder containing .npz files\n",
    "input_folder = \"data3fields_npz\"\n",
    "\n",
    "# Process each .npz file\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".npz\") and filename != 'ni_3D.npz':\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Different fields need same number of timesteps or else some timesteps have 3 sensors while others have 5 sensors\n",
    "        # hence different fields need same number of trajectories # else some trajectories have 3 sensors while others have 5 sensors, not gonna work.\n",
    "        # number of sensors need to be consistent across all time and across all trajectories\n",
    "\n",
    "        # manager.add_field(sensor_locs, n_params, n_trajectories???, ..., {id}) # since different fields need the same number of trajectories\n",
    "        # actually not necessarily... if field 1 has 1000 timesteps (think of multiple trajects just as more timesteps, though not the same especially with lag generation),\n",
    "        # then field 2 also needs 1000 timesteps for this to work right? Yes. Spatial shape can vary, but not timesteps I believe.\n",
    "\n",
    "        # but sensor locs and n_params shouldn't be in add_field because it might not have those (want to reconstruct a field we aren't measuring from a field we are measuring)\n",
    "        # though time and n_trajectories should remain constant across all fields.\n",
    "\n",
    "        # maybe:\n",
    "        # allow user to create SHREDDataProcessor object, add trajectories needed, then put into Manager. WAIT WHAT ABOUT TRAIN/VALID/TEST indices?\n",
    "        # \n",
    "\n",
    "        # manager.{id}.add_trajectory()\n",
    "        # BADDDDD SEE NOTES AND PHOTOS\n",
    "\n",
    "        # Add the file to the SHREDDataManager\n",
    "        manager.add_field(\n",
    "            file_path=[file_path, file_path, file_path], # each element is a trajectory. Manager ensures all trajectories are number of timesteps, and all fields have same number of trajectories\n",
    "            compression=20,\n",
    "            scaling=\"minmax\",\n",
    "            time=np.arange(0, 2000)  # Assuming all files have 2000 timesteps\n",
    "        )\n",
    "        print(f\"Processed: {filename}\")\n",
    "\n",
    "filename = 'ni_3D.npz'\n",
    "file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "manager.add_field(\n",
    "    file_path=[file_path, file_path, file_path],\n",
    "    random_sensors=3,\n",
    "    stationary_sensors=(7, 9),  # Example coordinates\n",
    "    mobile_sensors=sensor_perimeter_walk_coordinates,\n",
    "    compression=20,\n",
    "    scaling=\"minmax\",\n",
    "    time=np.arange(0, 2000)  # Assuming all files have 2000 timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = manager.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reconstructor.X.shape[0] + val_set.reconstructor.X.shape[0] + test_set.reconstructor.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be (1400, 21, 5)\n",
    "train_set.forecaster.X[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "shred = models.SHRED(train_set.reconstructor.X.shape[-1], train_set.reconstructor.Y.shape[-1], hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
    "validation_errors = models.fit(shred, train_set.reconstructor, val_set.reconstructor, batch_size=64, num_epochs=1000, lr=1e-3, verbose=True, patience=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = shred(test_set.reconstructor.X).detach().cpu().numpy()\n",
    "prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyshred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
